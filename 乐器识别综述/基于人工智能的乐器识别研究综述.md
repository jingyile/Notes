#  基于人工智能的乐器识别研究综述



# 背景

## 研究意义

随着数据分析和计算机科学的发展，机器学习和人工智能技术在图像、音频、视频等处理和分析中得到广泛应用。

在声学信号分析领域，乐器识别是音乐信息检索中的一项基础性工作。

基本思想是：让计算机系统“听”音乐声音，利用声学信号提取感知相关特征，并识别正在演奏的乐器。

这项工作具有音乐学和计算机科学的双重学术意义。

## 现状和存在问题

相比于图像处理技术，音频处理还没有像VGG, ResNet 一样日趋成熟的通用算法模型。

在音频处理领域，不同于歌曲分类和听歌识曲，乐器识别又有其独特的难点：

1.问题描述和相应评判标准的复杂性

2.数据和资源对于标注的高要求性

3.研究思路和方法的各异性

乐器识别的研究热点：

1.基于独奏音频或单个音符的乐器识别

2.合奏音乐的声源分离

3.基于合奏音乐的多分类乐器识别

独立音符和单音声音的乐器识别已经有了许多成功的音乐数据特征提取和选择方法

然而，这些算法大都不能直接用于复调音乐。

多声道音乐结构的识别困难且具有挑战性。

要分析合奏音乐中的乐器，需要对音频进行声源分离将其转化为单音轨

或利用人工智能来学习音频特征以直接完成分类。

# 音频特征提取方法



## 常用特征提取方法



### **线性预测分析（LPC)**

线性预测分析的基本思想是：

由于语音样点之间存在相关性，所以可以用过去的样点值来预测现在或将来的样点值，

即一个语音抽样可以用过去若干个语音抽样或它们的线性组合来逼近。

通过使实现语音抽样与线性预测抽样之间的误差在某个准则（通常为最小均方误差准则）

下达到最小值来决定一组预测系数。

这一组预测系数就反映了语音信号的特性，可以作为语音信号的特征参数用于语音合成和语音识别等。

**感知线性预测系数（PLP）**

**Tandem特征**

**Bottleneck特征**

**基于滤波器组的Fbank特征**



### **线性预测倒谱系数（LPCC）**

LPCC是LPC在倒谱域中的表示

假设语音信号为自回归信号，利用线性预测分析获得倒谱系数。

- 本质：声道可以进行线性假设

- 原因：LPC对误差比较敏感（LPC误差滤波器，误差本身就是很小的数值，较容易被影响），导致小的误差也会使频谱质量下降

- 过程：LPCC是LPC系数在倒谱中的表示。LPC系数的z变换的对数模函数的反Z变换

- 与LPC对比：可以对倒谱系数进行倒谱平均减（CMS）以消除信道效应

- 优点：计算量小，易于实现，元音描述的好，还可以描述共振峰，去除激励信息

- 缺点：对辅音描述能力弱，因为线性假设针对辅音不成立，抗噪性能差

  

### **梅尔倒谱系数（MFCC）**

- 实质：听觉感知（更符合人的听觉范围），非线性。
- 根据人耳听觉机理的研究发现，人耳对不同频率的声波有不同的听觉敏感度。从200Hz到5000Hz的语音信号对语音的清晰度影响对大。

> 这种参数比基于声道模型的LPCC相比具有更好的鲁邦性，更符合人耳的听觉特性，而且当信噪比降低时仍然具有较好的识别性能。
>
> 一般来说，信噪比越大，说明混在信号里的噪声越小，声音回放的音质量越高，否则相反。

- 过程：过程类似倒谱分析，只是将频率f转换为梅尔频率m
- 与LPCC对比：MFCC更符合人的听觉特性；LPCC不适合分析辅音，MFCC可以

## 拓展：计算机三大变化

### 傅立叶变换

表示能将满足一定条件的某个函数表示成三角函数（正弦和/或余弦函数）或者它们的积分的线性组合。

在不同的研究领域，傅立叶变换具有多种不同的变体形式，

如连续傅立叶变换和离散傅立叶变换。

最初傅立叶分析是作为热过程的解析分析的工具被提出的。

### 拉普拉斯变换

工程数学中常用的一种积分变换，又名拉氏变换。 

拉氏变换是一个线性变换，可将一个有参数实数t（t≥ 0）的函数转换为一个参数为复数s的函数。

拉普拉斯变换在许多工程技术和科学研究领域中有着广泛的应用，

特别是在力学系统、电学系统、自动控制系统、可靠性系统以及随机服务系统等系统科学中都起着重要作用。

### Z变换

Z变换（英文：z-transformation）可将时域信号（即：离散时间序列）变换为在复频域的表达式。

它在离散时间信号处理中的地位，如同拉普拉斯变换在连续时间信号处理中的地位。

离散时间信号的Z变换是分析线性时不变离散时间系统问题的重要[工具](https://www.seotest.cn/wenzhang/gongju/)，

在数字信号处理、计算机控制系统等领域有着广泛的应用。

# 乐器识别方法

## 基于机器学习的乐器识别方法



### SVM

支持向量机（support vector machines, SVM）是一种二分类模型，有监督学习！

它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；

SVM还包括**核技巧**，这使它成为实质上的非线性分类器。

所谓的核技巧，即两样本在特征变换后的特征内积，可直接通过两样本原始特征内积在某函数（核函数）的变换得到。

SVM的的学习策略就是间隔最大化（几何间隔：即点到面的距离）

SVM将低维线性不可分的空间转换为高维线性可分空间！

![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\P1.png)

![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\P2.png)

寻找与线最近的样本点→人眼感知最近的样本点与直线的几何间距→根据前面两步获得的信息判断分隔效果，

间距最大的即为最好。SVM就是遵循了这样一种判别思想。

![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\P3.png)



超级通俗易懂的例子：https://www.zhihu.com/question/21094489

### 随机森林模型

鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，

我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，

如果用全样本去训练m棵决策树显然是不可取的，

全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的

产生n个样本的方法采用Bootstraping法，这是一种有放回的抽样方法，产生n个样本

而最终结果采用Bagging的策略来获得，即多数投票机制



随机森林的生成方法：

1.从样本集中通过重采样的方式产生n个样本

2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点

3.重复m次，产生m棵决策树

4.多数投票机制来进行预测

总结：

随机森林是一个比较优秀的模型，它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。

运行效率和准确率较高，实现起来也比较简单。

但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。

### K最邻分类算法

K最近邻（KNN，K-NearestNeighbor）分类算法，是比较经典的分类算法，是将数据集合中每一个记录进行分类的方法，属于懒惰性学习算 法，只有当需要分类的向量到达时才开始构造泛化模型。是数据挖掘分类技术中最简单的方法之一。

算法中的每个样本都可以用它最接近的K个邻近值来代表。KNN算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。

**基本原理**

在样本集中找出与 ***\*待分类向量 tVec\**** 最相似的 k 个向量，然后***\*统计这 k 个向量中 出现次数最多的类别\****，把 tVec 归属为此类。KNN算法主要涉及***\*样本集、相似度的衡量、k 大小\****3个因素。

- 样本集也被称为训练集，是带有类别属性的向量集合；
- 两个向量的相似度一般通过计算它们的欧氏距离或余弦 度来衡量；
- k 的大小直接影响KNN算法的时空效率，如果 k 取值太小则容易受噪声的影响，k 取值过大，则近 邻中可能又包含过多其他类的数据点，因此一般情况 下， k 的取值一般不大于样本集的平方根。

**KNN算法流程**

***\*步骤1\**** 准备并预处理数据，即把样本数据和待分类数据采用合适的数据结构存储起来。

***\*步骤2\**** 设定参数 k ，从样本集随机抽取 k 个向量用来初始化大小为 k 的优先级队列，

即分别计算 tVec 到这 k 个向量的欧氏距离(dis)，然后把样本向量及其 dis存入优先级队列。

欧氏距离的计算公式为：（其中 xs、 xt 分别代表 d 维的样本向量和待分类向量， i 表示第 i 维的分量。 ）

![img](https://img-blog.csdnimg.cn/20200716170711974.png)

***\*步骤3\**** 遍历样本集中每个向量到 tVec 的 dis ，并将 dis 与优先级队列中的最大距离 (disMax) 比较大小， 若 dis<disMax ，则 disMax 及对应的向量出队，当前被 遍历到的向量及其 dis插入到优先级队列；否则，忽略当 前向量，继续在样本集中遍历下一个向量。

***\*步骤4\**** 遍历完毕得到与 tVec 距离最近的 k 个样本 向量集合（topK），统计出 k 个近邻中包含向量个数最多的类别，并把 tVec 归属到此类。

### 高斯混合模型

高斯混合模型可以看作是由 K 个单高斯模型组合而成的模型，这 K 个子模型是混合模型的隐变量（Hidden variable）。

一般来说，一个混合模型可以使用任何概率分布，

这里使用高斯混合模型是因为高斯分布具备很好的数学性质以及良好的计算性能。

举个不是特别稳妥的例子，比如我们现在有一组狗的样本数据，不同种类的狗，体型、颜色、长相各不相同，但都属于狗这个种类，此时单高斯模型可能不能很好的来描述这个分布，因为样本数据分布并不是一个单一的椭圆，所以用混合高斯分布可以更好的描述这个问题，如下图所示：

![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\P4.png)

### 隐马尔可夫模型

马尔可夫模型（Markov Model）是一种统计模型，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域。经过长期发展，尤其是在语音识别中的成功应用，使它成为一种通用的统计工具。

**什么是熵(Entropy)**

简单来说，熵是表示物质系统状态的一种度量，用它表征系统的无序程度。

熵越大，系统越无序，意味着系统结构和运动的不确定和无规则；

反之，，熵越小，系统越有序，意味着具有确定和有规则的运动状态。

熵的中文意思是热量被温度除的商。负熵是物质系统有序化，组织化，复杂化状态的一种度量。

熵最早来原于**物理学**. 德国物理学家鲁道夫·克劳修斯首次提出熵的概念，用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大。

1. 一滴墨水滴在清水中，部成了一杯淡蓝色溶液
2. 热水晾在空气中，热量会传到空气中，最后使得温度一致

更多的一些生活中的例子:

1. 熵力的一个例子是耳机线，我们将耳机线整理好放进口袋，下次再拿出来已经乱了。让耳机线乱掉的看不见的“力”就是熵力，耳机线喜欢变成更混乱。
2. 熵力另一个具体的例子是弹性力。一根弹簧的力，就是熵力。 胡克定律其实也是一种熵力的表现。
3. 万有引力也是熵力的一种(热烈讨论的话题)。

于是从微观看，熵就表现了这个系统所处状态的**不确定性程度**。香农，描述一个信息系统的时候就借用了熵的概念，这里熵表示的是这个信息系统的**平均信息量**(平均不确定程度)。

**最大熵模型**

我们在投资时常常讲不要把所有的鸡蛋放在一个篮子里，这样可以降低风险。

在信息处理中，这个原理同样适用。

在数学上，这个原理称为最大熵原理(the maximum entropy principle)。

让我们看一个拼音转汉字的简单的例子。假如输入的拼音是"wang-xiao-bo"，利用语言模型，根据有限的上下文(比如前两个词)，我们能给出两个最常见的名字“王小波”和“王晓波 ”。至于要唯一确定是哪个名字就难了，即使利用较长的上下文也做不到。当然，我们知道如果通篇文章是介绍文学的，作家王小波的可能性就较大；而在讨论两岸关系时，台湾学者王晓波的可能性会较大。

在上面的例子中，我们只需要综合两类不同的信息，即主题信息和上下文信息。

虽然有不少凑合的办法，

比如：分成成千上万种的不同的主题单独处理，或者对每种信息的作用加权平均等等，但都不能准确而圆满地解决问题。

数学上最漂亮的办法是最大熵(maximum entropy)模型，它相当于行星运动的椭圆模型。

“最大熵”这个名词听起来很深奥，但是它的原理很简单，我们每天都在用。

说白了，就是要保留全部的不确定性，将风险降到最小。

回到刚才的例子，我们已知两种信息，

第一，根据语言模型，wangxiao-bo可以被转换成王晓波和王小波；

第二，根据主题，王小波是作家，《黄金时代》的作者等等，而王晓波是台湾研究两岸关系的学者。

因此，我们就可以建立一个最大熵模型，同时满足这两种信息。现在的问题是，这样一个模型是否存在。

匈牙利著名数学家、信息论最高奖香农奖得主希萨（Csiszar）证明，

对任何一组不自相矛盾的信息，这个最大熵模型不仅存在，而且是唯一的。

而且它们都有同一个非常简单的形式 -- 指数函数。

下面公式是根据上下文（前两个词）和主题预测下一个词的最大熵模型，其中 w3 是要预测的词（王晓波或者王小波）w1 和 w2 是它的前两个字（比如说它们分别是“出版”，和“”），也就是其上下文的一个大致估计，subject 表示主题。

![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\公式1.jpg)

我们看到，在上面的公式中，有几个参数 lambda 和 Z ，他们需要通过观测数据训练出来。

最大熵模型在形式上是最漂亮的统计模型，而在实现上是最复杂的模型之一。

用最大熵模型可以将各种信息综合在一起。

而**如何构造最大熵模型**？

我们已经所有的最大熵模型都是指数函数的形式，现在只需要确定指数函数的参数就可以了，这个过程称为模型的训练。

最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代 算法。

GIS 的原理并不复杂，大致可以概括为以下几个步骤：

1. 假定第零次迭代的初始模型为等概率的均匀分布。

2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，

   如果超过了实际的，就把相应的模型参数变小；

   否则，将它们便大。

3. 重复步骤 2 直到收敛。

   

*再来看几个常用概念*

- 将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；

​     若给定若干随机变量，则形成一个有向图，即构成一个**网络**。

- 如果该网络是有向无环图，则这个网络称为**贝叶斯网络。**

- 如果这个图退化成线性链的方式，则得到**马尔可夫模型**；

  因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是**马尔可夫过程**。

- 若上述网络是无向的，则是无向图模型，又称**马尔可夫随机场或者马尔可夫网络**。

  ![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\图片1.png)

  马尔可夫过程（Markov process）是一类随机过程。

  它的原始模型马尔可夫链，由俄国数学家A.A.马尔可夫于1907年提出。

  该过程具有如下特性：在已知目前状态（现在）的条件下，它未来的演变（将来）不依赖于它以往的演变 (过去 )。

  例如森林中动物头数的变化构成——马尔可夫过程。

  在现实世界中，有很多过程都是马尔可夫过程，如液体中微粒所作的布朗运动、传染病受感染的人数、车站的候车人数等，都可视为马尔可夫过程。

  每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。

  最简单的马尔可夫过程就是一阶过程，**每一个状态的转移只依赖于其之前的那一个状态**，这个也叫作**马尔可夫性质**。

  假设这个模型的每个状态都只依赖于之前的状态，这个假设被称为**马尔科夫假设**，这个假设可以大大的简化这个问题。

  显然，这个假设可能是一个非常糟糕的假设，导致很多重要的信息都丢失了。

  - 假如今天是晴天，明天仍然是晴天的概率是0.9

  - 假如今天是晴天，明天变成阴天的概率是0.1，和上一条概率之和为1。

    ![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\P5.png)

    因此，一阶马尔可夫过程定义了以下三个部分：

    - **状态**：晴天和阴天
    - **初始向量**：定义系统在时间为0的时候的状态的概率
    - **状态转移矩阵**：每种天气转换的概率

  在某些情况下马尔科夫过程不足以描述我们希望发现的模式。

  回到之前那个天气的例子，一个隐居的人可能不能直观的观察到天气的情况，但是有一些海藻。

  民间的传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。

  在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气的状况）。

  我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。

  而这个算法就叫做**隐马尔可夫模型(HMM)**。

  

  隐马尔可夫模型 (Hidden Markov Model) 是一种统计模型，用来描述一个含有隐含未知参数的马尔可夫过程。

  **它是结构最简单的动态贝叶斯网，这是一种著名的有向图模型**，

  主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。



​    **隐马尔可夫三大问题**

1. 给定模型，如何有效计算产生观测序列的概率？

   换言之，如何评估模型与观测序列之间的匹配程度？

2. 给定模型和观测序列，如何找到与此观测序列最匹配的状态序列？

   换言之，如何根据观测序列推断出隐藏的模型状态？

3. 给定观测序列，如何调整模型参数使得该序列出现的概率最大？

   换言之，如何训练模型使其能最好地描述观测数据？

前两个问题是模式识别的问题：

1) 根据隐马尔科夫模型得到一个可观察状态序列的概率(**评价**)；

2) 找到一个隐藏状态的序列使得这个序列产生一个可观察状态序列的概率最大(**解码**)。

3) 第三个问题就是根据一个可以观察到的状态序列集产生一个隐马尔科夫模型（**学习**）。

对应的三大问题解法：

1. 向前算法(Forward Algorithm)、向后算法(Backward Algorithm)
2. 维特比算法(Viterbi Algorithm)
3. 鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于EM算法)

## 基于深度学习的乐器识别方法



### 多层感知机

### 卷积神经网络

### 循环神经网络

# 目前的问题和研究方向

目前的研究在独奏和单个音符的识别方面已经取得了丰富的成果，不少分类器都可以达到90%以上的准确度。

然而，合奏音乐的识别效果还不尽理想，乐器识别在时间维度上的反映灵敏度也没得到详细的研究论证。



研究问题：以合奏音乐的识别为主要突破点

拟研究过程：

1. 输入音乐片段，分析其主要演奏乐器
2. 输入音乐片段，判断是非出现某一种或多种乐器
3. 输入连续的数据流，系统实时判断当前演奏的一种或多种乐器

需要注意的问题：

模型在不同音质条件下的性能

模型在不同数量乐器合奏中的性能

模型在不同音频时长长下的性能

模型对音色相似乐器分类的准确度



数据集问题：目前很多开源数据集还在不断更新，未来将更进一步加大涵盖的乐器种类和曲风

为了更好地服务本土音乐研究，可以添加中国民族乐器以扩充数据。



中国的民族乐器种类繁多，是不可多得的非物质文化遗产。

民族乐器的识别分类，目的是让世界上更多的人能够了解少数民族乐器，使少数民族的音乐源远流长！

同时，少数音乐的AI作曲也是很值得去研究的方向。



![](C:\Users\Administrator\Desktop\Notes\乐器识别综述\图片.jpg)

